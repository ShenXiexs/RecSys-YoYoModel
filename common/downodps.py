# coding=utf-8
import os
import time
import gzip
import multiprocessing
import logging
import concurrent.futures
import argparse
import psutil
import pandas as pd
from odps import ODPS
from odps.tunnel import TableTunnel
from dateutil.parser import parse

from common.connect_config import *

try:
    from common.utils import schema, slots_dict, train_config as TrainConfig, compression_type, features_sep, field_sep, \
        seq_feat_in_dataset_pad, seq_features_index, seq_features_config, seq_idxs, seq_feat_pad
except:
    from utils import schema, slots_dict, train_config as TrainConfig

multiprocessing.log_to_stderr()
logger = multiprocessing.get_logger()
logger.setLevel(logging.INFO)
slots = list(slots_dict.keys())

def get_odps():
    """ é“¾æ¥odps """
    odps_obj = ODPS(your_accesskey_id, your_accesskey_secret, your_default_project, endpoint=your_end_point,
                    tunnel_endpoint=tunnel_endpoint)
    return odps_obj


def features_mapper(features):
    lst = features.strip("\n").split(features_sep)
    if len(lst) != len(schema):
        return ""
    return features_sep.join([v for k, v in zip(schema, lst) if k in slots])


def new_features_mapper(features):
    lst = features.strip("\n").split(features_sep)
    # åœ¨ç‰¹å¾ä¸Šæ·»åŠ ä¸€ä¸ªåˆ—indexå‰ç¼€ï¼Œä¸»è¦æ˜¯ä¸ºäº†å…¨å±€hashæ—¶ï¼Œä¸åŒç‰¹å¾åŸŸç‰¹å¾å–å€¼ç›¸åŒæ—¶ï¼Œhashç»“æœå†²çªçš„æƒ…å†µ
    return features_sep.join(
        [f"{col_idx}:{fea_val}" for col_idx, (fea_name, fea_val) in enumerate(zip(schema, lst)) if fea_name in slots])


def seq_features_mapper(features):
    lst = features.strip("\n").split(features_sep)
    new_lst = []
    for i, (start, end, index) in enumerate(seq_idxs):
            new_lst.extend([f"{fea_val}" if fea_val == seq_feat_pad[i] else f"{index}:{fea_val}"
                            for fea_val in lst[start:end]
                            ])
    return features_sep.join(new_lst)


def write(file, batch, data_schema):
    df = batch.to_pandas()[data_schema]
    df["features"] = df["features"].map(lambda x: new_features_mapper(x))
    if seq_features_index:
        df["seq_features"] = df["seq_features"].map(lambda x: seq_features_mapper(x))
    df.to_csv(file, mode="a", sep=field_sep, compression=compression_type.lower(), index=False, header=None)


def task(idx_date, idx, start, step,
         data_dir,
         table_name,
         partitions,
         data_schema):
    time.sleep(1)
    logger.info(f'table_name:${table_name},partition:{partitions} idx_date: {idx_date} {idx} {start} {step}')
    t0 = time.time()
    #################################################################################
    odps = get_odps()
    tunnel = TableTunnel(odps)
    #################################################################################
    download_session = tunnel.create_download_session(table_name, partition_spec=partitions)
    file = f"{data_dir}/part-r-{str(idx).zfill(2)}-{start}-{step}.gz"
    with download_session.open_arrow_reader(start, step) as reader:
        for i, batch in enumerate(reader):
            write(file, batch, data_schema)
            if i % 100 == 0:
                logger.info(f"è¿›åº¦: {idx_date} {start} {step} {i} {i * 1024 / step * 100:.2f}%")

    t1 = time.time()
    return f'idx_date: {idx_date} {start} {step} waste: {(t1 - t0) // 60}'


def get_args():
    save_path = os.environ.get("DATA_ROOT", "/data/share/opt/data")
    parser = argparse.ArgumentParser(description='manual to this script')
    parser.add_argument("--start_date", default="", type=str, help="æ•°æ®ä¸‹è½½å¼€å§‹æ—¥æœŸ")
    parser.add_argument("--end_date", default="", type=str, help="æ•°æ®ä¸‹è½½ç»“æŸæ—¥æœŸ")
    parser.add_argument("--save_path", default=save_path, type=str, help="æ•°æ®ä¸‹è½½å­˜å‚¨è·¯å¾„")
    parser.add_argument("--max_workers", default=16, type=int, help="æ•°æ®ä¸‹è½½è¿›ç¨‹æ•°")
    parser.add_argument("--durations", default="1", type=str, help="æ•°æ®durationsåˆ†åŒº")
    parser.add_argument("--task", default="", type=str, help="ä»»åŠ¡å")
    args, unknown = parser.parse_known_args()
    print((args, unknown))
    return args


def wait_for_process_exit(
        target_script: str = "downodps.py",
        target_task: str = "task O35_mutil_cvr",
        check_interval: int = 60,  # æ¯æ¬¡æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰
        max_wait_seconds: int = 24 * 3600,  # æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆ24å°æ—¶ï¼‰
        **kwargs
) -> bool:
    """
    ç­‰å¾…ç›®æ ‡è¿›ç¨‹é€€å‡ºï¼šæ£€æµ‹åŒ…å«æŒ‡å®šè„šæœ¬åå’Œä»»åŠ¡åçš„è¿›ç¨‹ï¼Œå­˜åœ¨åˆ™æŒç»­ç­‰å¾…ï¼Œè¶…æ—¶è¿”å›False

    Args:
        target_script: ç›®æ ‡è„šæœ¬åï¼ˆå¦‚ "downodps.py"ï¼‰
        target_task: ç›®æ ‡ä»»åŠ¡æ ‡è¯†ï¼ˆå¦‚ "task O35_mutil_cvr"ï¼‰
        check_interval: æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰
        max_wait_seconds: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        bool: è¿›ç¨‹é€€å‡ºè¿”å›Trueï¼›è¶…æ—¶æœªé€€å‡ºè¿”å›False
    """
    start_time = time.time()  # è®°å½•å¼€å§‹ç­‰å¾…æ—¶é—´
    current_pid = os.getpid()  # è·å–å½“å‰Pythonç¨‹åºçš„PID
    while True:
        # è®¡ç®—å·²ç­‰å¾…æ—¶é—´
        elapsed_time = time.time() - start_time
        if elapsed_time > max_wait_seconds:
            print(f"âš ï¸  ç­‰å¾…è¶…æ—¶ï¼ˆå·²è¶…è¿‡ {max_wait_seconds / 3600:.1f} å°æ—¶ï¼‰ï¼Œç›®æ ‡è¿›ç¨‹ä»åœ¨è¿è¡Œï¼Œé€€å‡ºç­‰å¾…")
            return False
        # æ ‡è®°æ˜¯å¦æ‰¾åˆ°ç›®æ ‡è¿›ç¨‹
        process_found = False
        # æšä¸¾æ‰€æœ‰è¿›ç¨‹ï¼Œæ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        for proc in psutil.process_iter(["pid", "cmdline"]):
            try:
                # è·å–è¿›ç¨‹çš„å‘½ä»¤è¡Œåˆ—è¡¨ï¼ˆè¿‡æ»¤æ— æ•ˆè¿›ç¨‹ï¼‰
                cmdline = proc.info.get("cmdline", [])
                # å°†å‘½ä»¤è¡Œåˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²ï¼ˆä¾¿äºåŒ¹é…ï¼‰
                try:
                    cmd_str = " ".join(cmdline)
                except Exception as e:
                    print(f" e:{e} , cmdline={cmdline} ")
                    continue
                #
                proc_pid = proc.info["pid"]
                # è·³è¿‡å½“å‰è¿›ç¨‹ï¼ˆé¿å…è¯¯åˆ¤è‡ªèº«ï¼‰
                if proc_pid == current_pid:
                    print(f"cmd={cmd_str}, pid={proc_pid}, current_pid={current_pid}")
                    continue
                if not cmdline:
                    continue  # è·³è¿‡æ— å‘½ä»¤è¡Œçš„è¿›ç¨‹ï¼ˆå¦‚ç³»ç»Ÿè¿›ç¨‹ï¼‰
                # æ£€æŸ¥æ˜¯å¦åŒæ—¶åŒ…å«è„šæœ¬åå’Œä»»åŠ¡æ ‡è¯†ï¼ˆåŒ¹é…é€»è¾‘ä¸åŸpså‘½ä»¤ä¸€è‡´ï¼‰
                if target_script in cmd_str and target_task in cmd_str:
                    flag = True
                    for v in kwargs.values():
                        flag = v in cmd_str and flag
                    if flag:
                        print(f"âœ… æ‰¾åˆ°ç›®æ ‡è¿›ç¨‹ï¼ˆPID: {proc_pid}ï¼‰ï¼Œç»§ç»­ç­‰å¾…... "
                              f"ï¼ˆå·²ç­‰å¾… {elapsed_time / 3600:.1f} å°æ—¶ï¼Œ"
                              f"å‰©ä½™ {max(0, max_wait_seconds - elapsed_time) / 3600:.1f} å°æ—¶ï¼‰")
                        process_found = True
                        break  # æ‰¾åˆ°ä¸€ä¸ªå³å¯ï¼Œæ— éœ€ç»§ç»­éå†
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                # å¿½ç•¥æ— æƒé™è®¿é—®æˆ–å·²é€€å‡ºçš„è¿›ç¨‹
                continue
        # è‹¥æœªæ‰¾åˆ°ç›®æ ‡è¿›ç¨‹ï¼Œè¯´æ˜å·²é€€å‡ºï¼Œè¿”å›True
        if not process_found:
            print(f"ğŸ‰ ç›®æ ‡è¿›ç¨‹å·²é€€å‡ºï¼Œç»“æŸç­‰å¾…ï¼ˆæ€»ç­‰å¾…æ—¶é—´ï¼š{elapsed_time / 60:.1f} åˆ†é’Ÿï¼‰")
            return True
        # æ‰¾åˆ°è¿›ç¨‹ï¼Œç¡çœ æŒ‡å®šé—´éš”åç»§ç»­æ£€æµ‹
        time.sleep(check_interval)


if __name__ == '__main__':
    args = get_args()
    end_date = args.start_date
    if args.end_date and parse(args.end_date) > parse(args.start_date):
        end_date = parse(args.end_date).strftime('%Y%m%d')
    try:
        dates = pd.date_range(args.start_date, end_date).map(lambda x: x.strftime("%Y%m%d")).tolist()
    except:
        dates = TrainConfig.downodps_datas
    # å®šä¹‰æ¨¡å‹åˆ†æ”¯å’Œæ‹‰æ•°æ®å­˜å‚¨è·¯å¾„
    model_ver = getattr(TrainConfig, "data_nm", TrainConfig.model_version)
    data_root_dir = f'{args.save_path}/{model_ver}'
    feature_data_table_name = TrainConfig.binning_table_name
    executor = concurrent.futures.ProcessPoolExecutor(max_workers=args.max_workers)
    data_schema = TrainConfig.data_schema
    for day in dates:
        ###############add by huangmian##############
        wait_for_process_exit(
            target_script="downodps.py",
            target_task=f"task {TrainConfig.data_nm}",
            check_interval=60,  # æ¯æ¬¡æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰
            max_wait_seconds=20 * 3600,  # æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            start=f"--end_date {day}"
        )
        ##############################################
        basepath = f"{data_root_dir}/{day}"
        success = f"{basepath}/_SUCCESS"
        if os.path.exists(success):
            logger.info(f"success file exits: {success}")
            continue
        if not os.path.exists(basepath):
            os.makedirs(basepath)
        t0 = time.time()
        odps = get_odps()
        tunnel = TableTunnel(odps)
        table = odps.get_table(feature_data_table_name)
        #
        # partitions = f"idx_date={day},feature_version={model_ver},durations={args.durations}"
        partitions = TrainConfig.partitions.format(day=day)
        for i in range(1440):
            if table.exist_partition(partitions):
                break
            if i % 10 == 0:
                logger.info(f"table:{feature_data_table_name} partitions not exits: {partitions}")
            time.sleep(60)

        download_session = tunnel.create_download_session(feature_data_table_name, partition_spec=partitions)
        step = download_session.count // max(os.cpu_count() - 1, 1)
        start = 0
        idx = 0

        futures = []
        while start < download_session.count:
            cnt = min(download_session.count - start, step)
            future = executor.submit(task, day, idx, start, cnt, basepath,
                                     feature_data_table_name, partitions, data_schema)
            start += step
            idx += 1

            futures.append(future)

        for future in futures:
            result = future.result()
            logger.info(f"result: {result}")

        t1 = time.time()
        f = gzip.open(success, "w")
        f.write(f"{(t1 - t0) / 60:.2f}".encode())
        f.close()

    executor.shutdown()

